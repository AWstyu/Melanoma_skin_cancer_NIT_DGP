{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80cc88-318b-4a9e-ba28-2fe9509a35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\ML_PROJECTS')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12d848-10f8-4fc2-b059-89054f24c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename='vgg19_melanoma_10396.jpg_reduced_features_test.pkl'\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765ebb2-c6bc-4775-a298-4716e9a13aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42)),\n",
    "    ('xgboost', XGBClassifier(eval_metric='logloss', random_state=42)),\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "\n",
    "final_model = LogisticRegression(random_state=42)\n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=final_model)\n",
    "\n",
    "l=[]\n",
    "def get_feature_vector(image_name):\n",
    "    x=0\n",
    "    i=0\n",
    "    f1=f'vgg16_{image_name}_reduced_features.pkl'\n",
    "    with open(f1,'rb') as f:\n",
    "        data1=pickle.load(f)\n",
    "    f2=f'vgg19_{image_name}_reduced_features.pkl'\n",
    "    with open(f2,'rb') as f:\n",
    "        data2=pickle.load(f)\n",
    "    f3=f'resnet50_{image_name}_reduced_features.pkl'\n",
    "    with open(f3,'rb') as f:\n",
    "        data3=pickle.load(f)\n",
    "    f4=f'inceptionv3_{image_name}_reduced_features.pkl'\n",
    "    with open(f4,'rb') as f:\n",
    "        data4=pickle.load(f)\n",
    "    f5=f'densenet_features_concatenated.pkl'\n",
    "    with open(f5,'rb') as f:\n",
    "        data5=pickle.load(f)\n",
    "    f_v=get_f_v_densenet(data5,image_name)\n",
    "    for j in range(1024):\n",
    "        x=(data1[i][j]+data2[i][j]+data3[i][j]+data4[i][j]+f_v[j])/5\n",
    "        l.append(x)\n",
    "    return l    \n",
    "   \n",
    "        \n",
    "        \n",
    "def get_f_v_densenet(data5,image_name):\n",
    "    for img,f_v in data5:\n",
    "        if image_name in img:\n",
    "           return f_v       \n",
    "\n",
    "base_dir='C:\\\\ML_PROJECTS'\n",
    "\n",
    "label_0_dir=os.path.join(base_dir,'label_0')\n",
    "label_1_dir=os.path.join(base_dir,'label_1')\n",
    "\n",
    "def search_label_0_dir(filename):\n",
    "    for files in os.listdir(label_0_dir):\n",
    "       if filename in files:\n",
    "           return 1\n",
    "    return 0\n",
    "\n",
    "def search_label_1_dir(filename):\n",
    "    for files in os.listdir(label_1_dir):\n",
    "       if filename in files:\n",
    "           return 1\n",
    "    return 0\n",
    "\n",
    "df=pd.read_excel('melanoma_org_1.xlsx')\n",
    "label=[]\n",
    "for idx,row in df.iterrows():\n",
    "    image_name=row['image_name']\n",
    "    print(image_name)\n",
    "    filename=f'concatenated_features_{image_name}.pkl'\n",
    "    f_v=get_feature_vector(image_name)\n",
    "    arr=np.array(f_v)\n",
    "    print('Feature vector avg--->:')\n",
    "    print(arr)\n",
    "    print(f'Size:{arr.shape}')\n",
    "    x=search_label_0_dir(filename)\n",
    "    y=search_label_1_dir(filename)\n",
    "    print('label:\\t')\n",
    "    if x:\n",
    "        print(0)\n",
    "        label.append(0)\n",
    "    elif y:\n",
    "        print(1)\n",
    "        label.append(1)\n",
    "\n",
    "arr=np.array(f_v)\n",
    "\n",
    "print(arr.shape)\n",
    "\n",
    "reshaped_array=arr.reshape(2000,1024)\n",
    "\n",
    "stacked_model.fit(reshaped_array,label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87de38-9987-42c7-8783-f1ae3df2317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\ML_PROJECTS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74961acf-1581-40f8-bf81-7645866c0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "path_benign_test='C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\benign'\n",
    "path_malignant_test='C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\malignant'\n",
    "\n",
    "\n",
    "def load_images_with_labels(path,label,num_samples):\n",
    "    images=os.listdir(path)\n",
    "    if num_samples>len(images):\n",
    "        num_samples=len(images)\n",
    "    selected_images=random.sample(images,num_samples)    \n",
    "    labeled_images=[]\n",
    "    for img in selected_images:\n",
    "        img_path=os.path.join(path,img)\n",
    "        img_arr=cv2.imread(img_path)\n",
    "        if img_arr is not None:\n",
    "            labeled_images.append((img,label))\n",
    "    return labeled_images       \n",
    "\n",
    "benign_images=load_images_with_labels(path_benign_test,0,1000)\n",
    "malignant_images=load_images_with_labels(path_malignant_test,1,1000)\n",
    "\n",
    "data_images_mod=benign_images+malignant_images\n",
    "\n",
    "print(len(data_images_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb04146-0892-4505-97c2-37724efc93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(data_images_mod)\n",
    "\n",
    "for img,label in data_images_mod:\n",
    "    print(label,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee30b90-f6b6-479f-8766-95ce32465596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "df = pd.read_excel('melanoma_org.xlsx')\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "   \n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def process_batch(batch_df):\n",
    " \n",
    "    features = []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        img_name = row['image_name']\n",
    "        status = row['status']\n",
    "        if status==0:\n",
    "          img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\benign', img_name)\n",
    "        else:\n",
    "          img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\malignant', img_name)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        try:\n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            feature_vector = model.predict(img)\n",
    "            feature_vector = feature_vector.flatten()\n",
    "            features.append([img_name, feature_vector])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "all_features = []\n",
    "\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_features = process_batch(batch_df)\n",
    "    all_features.extend(batch_features)\n",
    " \n",
    "    intermediate_df = pd.DataFrame(batch_features, columns=['Image_name', 'Feature_vector'])\n",
    "    intermediate_df.to_pickle(f'vgg16_features_batch_test_mod_{i//batch_size}.pkl')\n",
    "    \n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cd8a2-9511-40ee-ae41-3e613d83934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "data_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    filename=f\"vgg16_features_batch_test_mod_{i}.pkl\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "concatenated_data = np.concatenate(data_list, axis=0)\n",
    "\n",
    "with open('vgg16_features_concatenated_test.pkl', 'wb') as f:\n",
    "    pickle.dump(concatenated_data, f)\n",
    "\n",
    "print(\"Concatenation complete and saved to 'vgg16_features_concatenated_test.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a2791-ca66-4bdf-991d-98a291d21ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "df = pd.read_excel('melanoma_org.xlsx')\n",
    "\n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "   \n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def process_batch(batch_df):\n",
    "   \n",
    "    features = []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        img_name = row['image_name']\n",
    "        status = row['status']\n",
    "        if status==0:\n",
    "          img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\benign', img_name)\n",
    "        else:\n",
    "          img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\malignant', img_name)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        try:\n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            feature_vector = model.predict(img)\n",
    "            feature_vector = feature_vector.flatten()\n",
    "            features.append([img_name, feature_vector])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "all_features = []\n",
    "\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_features = process_batch(batch_df)\n",
    "    all_features.extend(batch_features)\n",
    " \n",
    "    intermediate_df = pd.DataFrame(batch_features, columns=['Image_name', 'Feature_vector'])\n",
    "    intermediate_df.to_pickle(f'vgg19_features_batch_test_{i//batch_size}.pkl')\n",
    "   \n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4208f-f5a8-4ca1-944f-fd4ff2619474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    filename=f\"vgg19_features_batch_test_{i}.pkl\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "\n",
    "concatenated_data=np.concatenate(data_list,axis=0)\n",
    "\n",
    "with open('vgg19_features_concatenated_test.pkl','wb') as f:\n",
    "    pickle.dump(concatenated_data,f)\n",
    "\n",
    "print(\"Concatenation complete and saved to 'vgg19_features_concatenated_test.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e66df-5d54-4bb3-8093-954ba2b2afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "df = pd.read_excel('melanoma_org.xlsx')\n",
    "\n",
    "\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, pooling='avg')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    \n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def process_batch(batch_df):\n",
    "   \n",
    "    features = []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        img_name = row['image_name']\n",
    "        status = row['status']\n",
    "        if status == 0:\n",
    "            img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\benign', img_name)\n",
    "        else:\n",
    "            img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\malignant', img_name)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        try:\n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            feature_vector = model.predict(img)\n",
    "            feature_vector = feature_vector.flatten()\n",
    "            features.append([img_name, feature_vector])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "all_features = []\n",
    "\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_features = process_batch(batch_df)\n",
    "    all_features.extend(batch_features)\n",
    "   \n",
    "    intermediate_df = pd.DataFrame(batch_features, columns=['Image_name', 'Feature_vector'])\n",
    "    intermediate_df.to_pickle(f'densenet_features_batch_test_{i//batch_size}.pkl')\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "print(\"Feature extraction complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac5958-b2d9-4f4b-afb4-d29c3c5de0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    filename=f\"densenet_features_batch_test_{i}.pkl\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "\n",
    "concatenated_data=np.concatenate(data_list,axis=0)\n",
    "\n",
    "with open('densenet_features_concatenated_test.pkl','wb') as f:\n",
    "    pickle.dump(concatenated_data,f)\n",
    "\n",
    "print(\"Concatenation complete and saved to 'densenet_features_concatenated_test.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56bdadb-16c1-4f81-b7b5-f4b6b715e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "df = pd.read_excel('melanoma_org.xlsx')\n",
    "\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    features = []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        img_name = row['image_name']\n",
    "        status = row['status']\n",
    "        if status == 0:\n",
    "            img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\benign', img_name)\n",
    "        else:\n",
    "            img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\malignant', img_name)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        try:\n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            feature_vector = model.predict(img)\n",
    "            feature_vector = feature_vector.flatten()\n",
    "            features.append([img_name, feature_vector])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "batch_size = 200\n",
    "all_features = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_features = process_batch(batch_df)\n",
    "    all_features.extend(batch_features)\n",
    "    \n",
    "   \n",
    "    intermediate_df = pd.DataFrame(batch_features, columns=['Image_name', 'Feature_vector'])\n",
    "    intermediate_df.to_pickle(f'inceptionv3_features_batch_test_{i//batch_size}.pkl')\n",
    "    \n",
    "   \n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcce77-6a59-449c-85dd-71900f8b71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    filename=f\"inceptionv3_features_batch_test_{i}.pkl\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "\n",
    "concatenated_data=np.concatenate(data_list,axis=0)\n",
    "\n",
    "with open('inceptionv3_features_concatenated_test.pkl','wb') as f:\n",
    "    pickle.dump(concatenated_data,f)\n",
    "\n",
    "print(\"Concatenation complete and saved to 'inceptionv3_features_concatenated_test.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec702739-e3c2-4e5f-80f7-b1f537e5125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "df = pd.read_excel('melanoma_org.xlsx')\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    features = []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        img_name = row['image_name']\n",
    "        status = row['status']\n",
    "        if status == 0:\n",
    "            img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\benign', img_name)\n",
    "        else:\n",
    "            img_path = os.path.join('C:\\\\ML_PROJECTS\\\\melanoma_cancer_dataset\\\\test\\\\malignant', img_name)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        try:\n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            feature_vector = model.predict(img)\n",
    "            feature_vector = feature_vector.flatten()\n",
    "            features.append([img_name, feature_vector])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "batch_size = 200\n",
    "all_features = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_features = process_batch(batch_df)\n",
    "    all_features.extend(batch_features)\n",
    "    \n",
    "    \n",
    "    intermediate_df = pd.DataFrame(batch_features, columns=['Image_name', 'Feature_vector'])\n",
    "    intermediate_df.to_pickle(f'resnet50_features_batch_test_{i//batch_size}.pkl')\n",
    "    \n",
    "    \n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f366f8a-2e97-4563-86db-acfebdebd14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    filename=f\"resnet50_features_batch_test_{i}.pkl\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "\n",
    "concatenated_data=np.concatenate(data_list,axis=0)\n",
    "\n",
    "with open('resnet50_features_concatenated_test.pkl','wb') as f:\n",
    "    pickle.dump(concatenated_data,f)\n",
    "\n",
    "print(\"Concatenation complete and saved to 'resnet50_features_concatenated_test.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f02e6a-f43d-40a7-baf4-42e3fc74fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('resnet50_features_concatenated_test.pkl','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for images,f_v in data:\n",
    "    print(f'image:{images},{f_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd2145-82d1-4478-91f4-0cbbc2e50429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('densenet_features_concatenated.pkl','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for images,f_v in data:\n",
    "    print(f'image:{images},{f_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a8ff3-7cca-4872-9030-4f2eefe8644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('vgg16_features_concatenated.pkl','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for images,f_v in data:\n",
    "    print(f'image:{images},{f_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526b26a-bf19-407a-af3f-6a1841e14930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('vgg19_features_concatenated.pkl','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for images,f_v in data:\n",
    "    print(f'image:{images},{f_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c90f1d-e683-4e0c-82d8-ad29e52b3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('inceptionv3_features_concatenated.pkl','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for images,f_v in data:\n",
    "    print(f'image:{images},{f_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71b7c4-f242-4707-be9c-bbf4a1551e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "import numpy as np\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(210,210,3)))  \n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(1024, activation='relu'))\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1='inceptionv3_features_concatenated_test.pkl'\n",
    "with open(f1,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for image,f_v in data:\n",
    "    input_shape=f_v.shape\n",
    "    # print(input_shape[0])\n",
    "    original_array=np.random.rand(input_shape[0])\n",
    "    padded_array=np.pad(original_array,(0,1228),mode='constant')\n",
    "    num_samples=1\n",
    "    height=210\n",
    "    width=210\n",
    "    no_of_channels=3\n",
    "    reshaped_array = padded_array.reshape(num_samples, height, width, no_of_channels)\n",
    "    features=model2.predict(reshaped_array)\n",
    "    filename=f'inceptionv3_{image}_reduced_features_test.pkl'\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "       pickle.dump(features,f)\n",
    "    \n",
    "    print(f'Dimension reduced and features saved for {image} successfully to {filename}!!')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777c603-ea89-4824-8418-5689603663cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "import numpy as np\n",
    "\n",
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(182,183,3)))  \n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(1024, activation='relu'))\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1='resnet50_features_concatenated_test.pkl'\n",
    "with open(f1,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for image,f_v in data:\n",
    "    input_shape=f_v.shape\n",
    "    #print(input_shape[0])\n",
    "    original_array=np.random.rand(input_shape[0])\n",
    "    padded_array=np.pad(original_array,(0,115),mode='constant')\n",
    "    num_samples=1\n",
    "    height=183\n",
    "    width=183\n",
    "    no_of_channels=3\n",
    "    reshaped_array = padded_array.reshape(num_samples, height, width, no_of_channels)\n",
    "    features=model3.predict(reshaped_array)\n",
    "    filename=f'resnet50_{image}_reduced_features_test.pkl'\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "       pickle.dump(features,f)\n",
    "    \n",
    "    print(f'Dimension reduced and features saved for {image} successfully to {filename}!!')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bde0b-84be-4287-a90f-2125c555230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "import numpy as np\n",
    "\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(92,92,3)))  \n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "model1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(1024, activation='relu'))\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1='vgg16_features_concatenated_test.pkl'\n",
    "with open(f1,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for image,f_v in data:\n",
    "    input_shape=f_v.shape\n",
    "    # print(input_shape[0])\n",
    "    original_array=np.random.rand(input_shape[0])\n",
    "    padded_array=np.pad(original_array,(0,304),mode='constant')\n",
    "    num_samples=1\n",
    "    height=92\n",
    "    width=92\n",
    "    no_of_channels=3\n",
    "    reshaped_array = padded_array.reshape(num_samples, height, width, no_of_channels)\n",
    "    features=model1.predict(reshaped_array)\n",
    "    filename=f'vgg16_{image}_reduced_features_test.pkl'\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "       pickle.dump(features,f)\n",
    "    \n",
    "    print(f'Dimension reduced and features saved for {image} successfully to {filename}!!')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684a3c6-6cf3-49d8-823f-471ca8f23e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "import numpy as np\n",
    "\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(92,92,3)))  \n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "model1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(1024, activation='relu'))\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1='vgg19_features_concatenated_test.pkl'\n",
    "with open(f1,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "for image,f_v in data:\n",
    "    input_shape=f_v.shape\n",
    "    # print(input_shape[0])\n",
    "    original_array=np.random.rand(input_shape[0])\n",
    "    padded_array=np.pad(original_array,(0,304),mode='constant')\n",
    "    num_samples=1\n",
    "    height=92\n",
    "    width=92\n",
    "    no_of_channels=3\n",
    "    reshaped_array = padded_array.reshape(num_samples, height, width, no_of_channels)\n",
    "    features=model1.predict(reshaped_array)\n",
    "    filename=f'vgg19_{image}_reduced_features_test.pkl'\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "       pickle.dump(features,f)\n",
    "    \n",
    "    print(f'Dimension reduced and features saved for {image} successfully to {filename}!!')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99be3c-fd18-483b-97ab-c2541f26737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\ML_PROJECTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4530f2a-1243-4882-9b14-06ddcb90d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df=pd.read_excel(\"melanoma_org.xlsx\")\n",
    "l_f_test=[]\n",
    "def get_feature(image_name):\n",
    "    i=0\n",
    "    x=0\n",
    "    f_vgg16_test=f\"vgg16_{image_name}_reduced_features_test.pkl\"\n",
    "    f_vgg19_test=f\"vgg19_{image_name}_reduced_features_test.pkl\"\n",
    "    f_inceptionv3_test=f\"inceptionv3_{image_name}_reduced_features_test.pkl\"\n",
    "    f_resnet50_test=f\"resnet50_{image_name}_reduced_features_test.pkl\"\n",
    "    filename=\"densenet_features_concatenated_test.pkl\"\n",
    "    with open(f_vgg16_test,'rb') as f:\n",
    "        data_vgg16=pickle.load(f)\n",
    "    with open(f_vgg19_test,'rb') as f:\n",
    "        data_vgg19=pickle.load(f)\n",
    "    with open(f_inceptionv3_test,'rb') as f:\n",
    "        data_inceptionv3=pickle.load(f)\n",
    "    with open(f_resnet50_test,'rb') as f:\n",
    "        data_resnet50=pickle.load(f)\n",
    "    with open(filename,'rb') as f:\n",
    "        data_densenet=pickle.load(f)\n",
    "    f_v=get_densenet_test(data_densenet,image_name)\n",
    "    \n",
    "    for j in range(1024):\n",
    "        x=(data_vgg16[i][j]+data_vgg19[i][j]+data_inceptionv3[i][j]+data_resnet50[i][j]+f_v[j])/5\n",
    "        l_f_test.append(x)\n",
    "    print(f\"Feature_vector_merged for {image_name}:{x}\")\n",
    "    \n",
    "        \n",
    "\n",
    "def get_densenet_test(data_densenet,image_name):\n",
    "    for img,f_v in data_densenet:\n",
    "        if image_name in img:\n",
    "            return f_v\n",
    "        \n",
    "            \n",
    "l_test=[]\n",
    "count=0\n",
    "for idx,row in df.iterrows():\n",
    "    image_name=row['image_name']\n",
    "    label=row['status']\n",
    "    get_feature(image_name)\n",
    "    l_test.append(label)\n",
    "    print(label)\n",
    "    count+=1\n",
    "    if count==800:\n",
    "        break\n",
    "\n",
    "test_array=np.array(l_f_test)\n",
    "mod_test_array=test_array.reshape(800,1024)\n",
    "print('array reshaped succesfully')\n",
    "print(mod_test_array)\n",
    "print(l_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff279cd-72e0-4a8c-b6b2-b3316c8fcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42)),\n",
    "    ('xgboost', XGBClassifier(eval_metric='logloss', random_state=42)),\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "\n",
    "final_model = LogisticRegression(random_state=42)\n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=final_model)\n",
    "\n",
    "l=[]\n",
    "def get_feature_vector(image_name):\n",
    "    x=0\n",
    "    i=0\n",
    "    f1=f'vgg16_{image_name}_reduced_features.pkl'\n",
    "    with open(f1,'rb') as f:\n",
    "        data1=pickle.load(f)\n",
    "    f2=f'vgg19_{image_name}_reduced_features.pkl'\n",
    "    with open(f2,'rb') as f:\n",
    "        data2=pickle.load(f)\n",
    "    f3=f'resnet50_{image_name}_reduced_features.pkl'\n",
    "    with open(f3,'rb') as f:\n",
    "        data3=pickle.load(f)\n",
    "    f4=f'inceptionv3_{image_name}_reduced_features.pkl'\n",
    "    with open(f4,'rb') as f:\n",
    "        data4=pickle.load(f)\n",
    "    f5=f'densenet_features_concatenated.pkl'\n",
    "    with open(f5,'rb') as f:\n",
    "        data5=pickle.load(f)\n",
    "    f_v=get_f_v_densenet(data5,image_name)\n",
    "    for j in range(1024):\n",
    "        x=(data1[i][j]+data2[i][j]+data3[i][j]+data4[i][j]+f_v[j])/5\n",
    "        l.append(x)\n",
    "    return l    \n",
    "   \n",
    "        \n",
    "        \n",
    "def get_f_v_densenet(data5,image_name):\n",
    "    for img,f_v in data5:\n",
    "        if image_name in img:\n",
    "           return f_v       \n",
    "\n",
    "base_dir='C:\\\\ML_PROJECTS'\n",
    "\n",
    "label_0_dir=os.path.join(base_dir,'label_0')\n",
    "label_1_dir=os.path.join(base_dir,'label_1')\n",
    "\n",
    "def search_label_0_dir(filename):\n",
    "    for files in os.listdir(label_0_dir):\n",
    "       if filename in files:\n",
    "           return 1\n",
    "    return 0\n",
    "\n",
    "def search_label_1_dir(filename):\n",
    "    for files in os.listdir(label_1_dir):\n",
    "       if filename in files:\n",
    "           return 1\n",
    "    return 0\n",
    "\n",
    "df=pd.read_excel('melanoma_org_1.xlsx')\n",
    "label=[]\n",
    "for idx,row in df.iterrows():\n",
    "    image_name=row['image_name']\n",
    "    print(image_name)\n",
    "    filename=f'concatenated_features_{image_name}.pkl'\n",
    "    f_v=get_feature_vector(image_name)\n",
    "    arr=np.array(f_v)\n",
    "    print('Feature vector avg--->:')\n",
    "    print(arr)\n",
    "    print(f'Size:{arr.shape}')\n",
    "    x=search_label_0_dir(filename)\n",
    "    y=search_label_1_dir(filename)\n",
    "    print('label:\\t')\n",
    "    if x:\n",
    "        print(0)\n",
    "        label.append(0)\n",
    "    elif y:\n",
    "        print(1)\n",
    "        label.append(1)\n",
    "\n",
    "arr=np.array(f_v)\n",
    "\n",
    "print(arr.shape)\n",
    "\n",
    "reshaped_array=arr.reshape(2000,1024)\n",
    "\n",
    "stacked_model.fit(reshaped_array,label)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885e697-dc2b-4b16-b91e-f55676d42951",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=stacked_model.predict(mod_test_array)\n",
    "accuracy = accuracy_score(l_test, y_pred)\n",
    "print(f'Accuracy of the stacked model: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
